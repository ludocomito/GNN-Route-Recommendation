{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full preprocessing pipeline of the GeoLife dataset for the GNN Route Recommendation project\n",
    "author: Ludovico Comito (comito.1837155@studenti.uniroma1.it)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The GeoLife dataset contains GPS a total of 17.621 total trajectories of everyday trips over five years. For each trip, a datapoint is constituted by (latitude, longitude, timestamp). Among these users, 74 of them have also labelled them with the transportation mode they were using (walk, car, bike...). This dataset contains data from various cities, and we will focus on the city of Beijing, being the one that contains most of the data.\n",
    "\n",
    "For the sake of this project, we are interested only in the portion of data that is annotated, as we want to build a recommender system that takes into account also the desired mean of transport.\n",
    "\n",
    "Each entry of the final dataset will contain:\n",
    "- ID of the user\n",
    "- Path trajectory\n",
    "- Transportation mode\n",
    "- Timestamp\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: get the labelled trajectories from the raw dataset\n",
    "You can download the raw dataset from [this page](https://www.microsoft.com/en-us/download/details.aspx?id=52367)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Data folder contains one subfolder names with the ID of each user. Within each user folder we have a trajectory folders and (optionally) a labels.txt file, that contains labelling with transport mode.\n",
    "\n",
    "We are interesting in isolating only labelled trajectories, from which we will store latitude, longitude and timestamp.\n",
    "\n",
    "The data for each user will be then stored in a temporary csv file for further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'geolife-data/Data'\n",
    "dirlist = os.listdir(data_dir)\n",
    "\n",
    "label_dirs = []\n",
    "for dir in dirlist[1:]:\n",
    "  trajlist = os.listdir(data_dir + '/' + dir)\n",
    "  if 'labels.txt' in trajlist:\n",
    "    label_dirs.append(data_dir + '/' + dir)\n",
    "\n",
    "traj_columns = ['latitude','longitude','height','days_total','date','time']\n",
    "\n",
    "def year_plus(col):\n",
    "  return col.replace(year=col.year + 1)\n",
    "\n",
    "def get_trans_trip(record_dt,ref_df):\n",
    "    time_fit = (record_dt >= ref_df['Start Time']) & (record_dt <= ref_df['End Time'])\n",
    "    nmatch = time_fit.sum()\n",
    "    if nmatch == 0:\n",
    "        t_idx = None\n",
    "    else:\n",
    "        if nmatch > 1:\n",
    "            print('More than one mode match!')\n",
    "        t_idx = ref_df.loc[time_fit].iloc[0].name\n",
    "    return t_idx\n",
    "\n",
    "match_paths = []\n",
    "for ldirs in label_dirs:\n",
    "  all_traj = pd.DataFrame()\n",
    "  print('Processing: ' + ldirs)\n",
    "  print(ldirs)\n",
    "  user = ldirs[-3:]\n",
    "  trajpath = ldirs + '/Trajectory/'\n",
    "  traj_files = os.listdir(trajpath)\n",
    "  trip_trans = pd.read_csv(ldirs+'/labels.txt',sep='\\t')\n",
    "  trip_trans['Start Time'] = pd.to_datetime(trip_trans['Start Time'])\n",
    "  trip_trans['End Time'] = pd.to_datetime(trip_trans['End Time'])\n",
    "\n",
    "  trip_s_dates = trip_trans['Start Time'].dt.date.unique()\n",
    "  trip_e_dates = trip_trans['End Time'].dt.date.unique()\n",
    "  trip_a_dates = np.unique(np.append(trip_s_dates,trip_e_dates))\n",
    "  for tf in traj_files:\n",
    "    traj_df = (pd.read_csv(trajpath+tf\n",
    "                            ,skiprows=6\n",
    "                            ,usecols=[0,1,3,4,5,6]\n",
    "                            ,names=traj_columns)\n",
    "                .assign(record_dt = lambda x: pd.to_datetime(\n",
    "                                        x['date'] + ' ' + x['time']\n",
    "                                        ),\n",
    "                        user = user\n",
    "                        )\n",
    "                )\n",
    "    if traj_df['record_dt'].dt.date.isin(trip_a_dates).any():\n",
    "        print('matches possible')\n",
    "        traj_df['trans_trip'] = traj_df.apply(lambda x: get_trans_trip(x.record_dt,trip_trans),axis=1)\n",
    "        has_trip = ~(traj_df.trans_trip.isnull())\n",
    "        traj_df['trans_mode'] = np.nan\n",
    "        traj_df.loc[has_trip,'trans_mode'] = traj_df.loc[has_trip].apply(lambda x: trip_trans.loc[x.trans_trip,'Transportation Mode'],axis=1)\n",
    "        all_traj = pd.concat([all_traj,traj_df])\n",
    "  print(f'Saving trajectories for user {user}')\n",
    "  all_traj.to_csv('csvs/'+f'user_{user}_'+'_trajectories.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: isolate single trajectories for each user\n",
    "Now we have to face a problem: we have successfully separated trajectories for each user, but they are sored as a unique csv from which we have to isolate single trips.\n",
    "\n",
    "The following function separates single trips based on shifts of datetime of changes in transport mode. Single trips are then pickled and stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to parse date and time and return a datetime object\n",
    "def parse_datetime(date_str, time_str):\n",
    "    return datetime.strptime(f\"{date_str} {time_str}\", \"%Y-%m-%d %H:%M:%S\")\n",
    "\n",
    "def filter_and_store(base_dir, allowed_modes=['walk', 'car', 'bus']):\n",
    "    # Define the allowed transportation modes\n",
    "    allowed_modes = ['walk', 'car', 'taxi', 'bus']\n",
    "    csv_dirs = os.listdir(base_dir)\n",
    "\n",
    "    for user_csv in csv_dirs:\n",
    "        user_file = 'csvs/' + user_csv\n",
    "        user_df = pd.read_csv(user_file)\n",
    "        print(f'Processing {user_file}')\n",
    "        if len(user_df.columns) > 5:\n",
    "            user_df = user_df[['latitude','longitude','date','time','user','trans_mode']]\n",
    "            user_id = user_df['user'][0]\n",
    "            \n",
    "            # Extracting trajectories\n",
    "            trajectories = []\n",
    "            current_trajectory = []\n",
    "\n",
    "            prev_mode = None\n",
    "            prev_datetime = None\n",
    "            for _, row in user_df.iterrows():\n",
    "                if row['trans_mode'] not in allowed_modes:\n",
    "                    # Skip modes not in allowed_modes\n",
    "                    continue\n",
    "\n",
    "                current_datetime = parse_datetime(row['date'], row['time'])\n",
    "                if (prev_datetime is not None and current_datetime - prev_datetime > timedelta(hours=3)) or (prev_mode!=row['trans_mode']):\n",
    "                    # Check if current trajectory is long enough before adding\n",
    "                    if len(current_trajectory) >= 10:\n",
    "                        trajectories.append(current_trajectory)\n",
    "                    current_trajectory = []\n",
    "                \n",
    "                current_trajectory.append(row.tolist())\n",
    "                prev_mode = row['trans_mode']\n",
    "                prev_datetime = current_datetime\n",
    "\n",
    "            # Check the last trajectory\n",
    "            if current_trajectory and len(current_trajectory) >= 10:\n",
    "                trajectories.append(current_trajectory)\n",
    "                \n",
    "            print(f'Saving pickled_data/array_{user_id}.pkl')\n",
    "            # Pickling the array\n",
    "            with open(f'pickled_data/array_{user_id}.pkl', 'wb') as file:\n",
    "                pickle.dump(trajectories, file)\n",
    "\n",
    "filter_and_store('csvs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: preprocess single trajectories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Raw GPS trajectories are tipically noisy, so we will preprocess them in the following way:\n",
    "1. Outlier removal.\n",
    "2. Datapoints sumsampling (compression).\n",
    "3. Staypoints and duplicates removal.\n",
    "4. Map matching.\n",
    "\n",
    "For outlier removals and compression we will use the skmob library.\n",
    "\n",
    "After these processing, we will have clean representation of our trips on the map. To make our data usable by the model, we have to apply two other steps:\n",
    "\n",
    "**Making paths continuous**\n",
    "\n",
    "Our architecture needs to have continous trajectories, meaning that the node at step k+1 must be a neighbour of node at step k (there needs to be an edge between them). To achieve this, we will simply connect non-neighbouring nodes by computing the shortest path between them using the Networkx library.\n",
    "\n",
    "**Cutting length**\n",
    "\n",
    "After preprocessing, our trips are still very long compared to the dataset that was used in the original NeuroMLR paper. A simple data analysis shows that the average length of nodes in a trip for the original dataset is 36, with a standard deviation of 34. In our data we have an average length of 243 with a standard deviation of 787 resulting in a far more challenging setting. To make our dataset comparable, trips are cut to a maximum length of 70.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from osmnx.distance import nearest_edges\n",
    "import skmob\n",
    "from skmob.preprocessing import filtering, compression, detection\n",
    "import geopandas as gpd\n",
    "import haversine\n",
    "from haversine import haversine, Unit\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load graph, nodes, and edges\n",
    "G = pickle.load(open('map/graph_with_haversine.pkl', 'rb'))\n",
    "edge_df = gpd.read_file('mlr_data/edges.shp')\n",
    "node_df = gpd.read_file('mlr_data/nodes.shp')\n",
    "a = node_df['osmid'].to_numpy()\n",
    "b = node_df[['y', 'x']].to_numpy()\n",
    "map_node_osm_to_coords = {e:(u,v) for e,(u,v) in zip(a,b)} # osmid to coords\n",
    "del a\n",
    "del b\n",
    "\n",
    "map_edge_id_to_u_v = edge_df[['u', 'v']].to_numpy()\n",
    "map_u_v_to_edge_id = {(u,v):i for i,(u,v) in enumerate(map_edge_id_to_u_v)}\n",
    "unique_edge_labels = list(map_u_v_to_edge_id.values())\t\t# these are from OSM map data, not train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_edges_to_ids(edges, map_u_v_to_edge_id):\n",
    "    return [map_u_v_to_edge_id[(u,v)] for u,v,i in edges]\n",
    "    \n",
    "def remove_consecutive_duplicates(numbers):\n",
    "    # This function removes consecutive duplicates from a list while preserving order\n",
    "    if not numbers:\n",
    "        return []\n",
    "\n",
    "    # Initialize the result list with the first element\n",
    "    result = [numbers[0]]\n",
    "\n",
    "    # Iterate over the list, compare each element with the last element in result\n",
    "    for num in numbers[1:]:\n",
    "        if num != result[-1]:\n",
    "            result.append(num)\n",
    "\n",
    "    return result\n",
    "\n",
    "def remove_stay_points(data, distance_threshold):\n",
    "    cleaned_latitudes = []\n",
    "    cleaned_longitudes = []\n",
    "\n",
    "    previous_point = (data['Latitude'][0], data['Longitude'][0])\n",
    "    cleaned_latitudes.append(previous_point[0])\n",
    "    cleaned_longitudes.append(previous_point[1])\n",
    "\n",
    "    for lat, lon in zip(data['Latitude'][1:], data['Longitude'][1:]):\n",
    "        current_point = (lat, lon)\n",
    "        dist = haversine(previous_point, current_point, unit=Unit.METERS)\n",
    "        print(f'dist: {dist}m')\n",
    "        if dist >= distance_threshold:\n",
    "            cleaned_latitudes.append(lat)\n",
    "            cleaned_longitudes.append(lon)\n",
    "            previous_point = current_point\n",
    "\n",
    "    return {'Latitude': cleaned_latitudes, 'Longitude': cleaned_longitudes}\n",
    "\n",
    "def build_path(G, edges):\n",
    "    path = []\n",
    "    for i in range(len(edges) - 1):\n",
    "        start_edge = edges[i]\n",
    "        end_edge = edges[i + 1]\n",
    "        # Find the shortest path between the nodes of consecutive nearest edges\n",
    "        try:\n",
    "            shortest_path = nx.shortest_path(G, source=start_edge[1], target=end_edge[0])\n",
    "            path.extend(shortest_path)\n",
    "        except nx.NetworkXNoPath:\n",
    "            print(\"No path\", start_edge, end_edge)\n",
    "    return path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_dir = 'pickled_data/'\n",
    "dirs = os.listdir(base_dir)\n",
    "\n",
    "processed_trajectories = []\n",
    "for user_file in dirs:\n",
    "    if user_file == '.DS_Store':\n",
    "        continue\n",
    "    print(f'processing: {user_file}')\n",
    "    user_dir = base_dir + user_file\n",
    "    with open(user_dir, 'rb') as file:\n",
    "        user_trajectories = pickle.load(file)\n",
    "    for trajectory in user_trajectories:\n",
    "        user_traj_df = pd.DataFrame(trajectory)\n",
    "        user_traj_df.columns = ['Latitude', 'Longitude', 'Date','Time','User_ID','Transport_mode'] \n",
    "        \n",
    "        # Convert to skmob format\n",
    "        user_traj_df['datetime'] = pd.to_datetime(user_traj_df['Date'] + ' ' + user_traj_df['Time'])\n",
    "\n",
    "        # Remove staypoints\n",
    "        user_traj_df = remove_stay_points(user_traj_df, 5)\n",
    "        \n",
    "        # Load skmob\n",
    "        tdf = skmob.TrajDataFrame(user_traj_df, latitude='Latitude', longitude='Longitude', datetime='datetime',user_id='User_ID')\n",
    "        # Filter outliers\n",
    "        ftdf = filtering.filter(tdf, max_speed_kmh=500.)\n",
    "        # Compress trajectory\n",
    "        ctdf = compression.compress(ftdf, spatial_radius_km=0.1)\n",
    "        \n",
    "        # Map match edges\n",
    "        edge_ids = nearest_edges(G, ctdf['lng'], ctdf['lat'])\n",
    "        \n",
    "        indexed_eges = map_edges_to_ids(edge_ids, map_u_v_to_edge_id)\n",
    "        indexed_eges = remove_consecutive_duplicates(indexed_eges)\n",
    "\n",
    "        # Make path continuous\n",
    "        indexed_edges = build_path(G, indexed_eges)\n",
    "        \n",
    "        # Normalize distance\n",
    "        indexed_edges = indexed_edges[:min(70, len(indexed_edges))]\n",
    "\n",
    "        # Store data\n",
    "        final_data = (ctdf['uid'][0], indexed_eges, ctdf['Transport_mode'][0],ctdf['datetime'][0]) # user_id, trajectory_edges, transport_mode, starting datetime\n",
    "        processed_trajectories.append(final_data)\n",
    "\n",
    "# save pickled processed trajectories\n",
    "with open('to_edges/processed_trajectories_all.pkl', 'wb') as file:\n",
    "    pickle.dump(processed_trajectories, file)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: preparing data for the model\n",
    "In order to reduce the cost of computation during training, we will preprocess the data in order to obtain a tensor dataset that we can feed straight to the model.\n",
    "\n",
    "- User ids: this is already numeric data, so we will only have to transform data into tensors.\n",
    "- Transport modes: we have three transport modes that are Walk, Car and Taxi. In this case we simply encode them into integers 0, 1, 2 using a dict.\n",
    "- Trajectories: we don't need any further preprocessing.\n",
    "\n",
    "**Timestamps**\n",
    "\n",
    "While preprocessing for the other data is pretty straightforward, we need to put attention in the choice of the useful information about timestamps and how to encode it. Raw timestamps are in the format YEAR:MONTH:DAY, HOUR:MINUTES:SECONDS. In my design choices, I decided to just keep the month, day of the week and hour. This is because the year seems less correlated to the choice of the path, as well as the specific minute and second that we are deciding. Instead month can suggest the season we are in, the day of the week can tell us whether we are in a work day or in the weekend and the hour can tell us about different choices of our user when travelling by day or night. These informations seem very meaningful to extract patterns.\n",
    "\n",
    "There's another choice to be addressed: how to encode time data in a meaningful way. We could simply choose to select the integers that represents months, weekdays and hours. But let's make an example: midnight is represented by the number 12 and one in the morning by number 1. These hours are close, but their numeric representation is bery different. In order to get a better representation, we will use sinusoidal encodings, that exploit the cyclical nature of our data by means of a sine/cosine representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transport_mode_dict = {\n",
    "    'walk': 0,\n",
    "    'car': 1,\n",
    "    'bus': 2,\n",
    "}\n",
    "\n",
    "def encode_transport_mode(mode, transport_mode_dict):\n",
    "    return transport_mode_dict[mode]\n",
    "\n",
    "def cyclical_encode(data, max_val):\n",
    "    sin_encoded = torch.sin(2 * np.pi * data / max_val)\n",
    "    cos_encoded = torch.cos(2 * np.pi * data / max_val)\n",
    "    return sin_encoded, cos_encoded\n",
    "\n",
    "def encode_timestamp(timestamp):\n",
    "    month = timestamp.month\n",
    "    hour = timestamp.hour\n",
    "    weekday = timestamp.weekday()\n",
    "\n",
    "    month_sin, month_cos = cyclical_encode(torch.tensor(month), 12)\n",
    "    hour_sin, hour_cos = cyclical_encode(torch.tensor(hour), 24)\n",
    "    weekday_sin, weekday_cos = cyclical_encode(torch.tensor(weekday), 7)\n",
    "\n",
    "    return torch.stack((month_sin, month_cos, hour_sin, hour_cos, weekday_sin, weekday_cos))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for elem in processed_trajectories:\n",
    "    user_id, trajectory_edges, transport_mode, timestamp = elem\n",
    "    # Encode user_id\n",
    "    elem[0] = torch.tensor([user_id])\n",
    "    # Encode transport mode\n",
    "    elem[2] = torch.tensor([encode_transport_mode(transport_mode, transport_mode_dict)])\n",
    "\n",
    "    # Encode timestamp\n",
    "    elem[3] = encode_timestamp(timestamp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final step: creating the dataset\n",
    "Finally, we will have to split our data into train, validation and test dataset. We will use 80% of the data for training 10% for validation and 10% for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First shuffle the data\n",
    "processed_trajectories = np.random.shuffle(processed_trajectories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split data into train, validation and test (0.8,0.1,0.1)\n",
    "train_data = processed_trajectories[:int(0.8 * len(processed_trajectories))]\n",
    "val_data = processed_trajectories[int(0.8 * len(processed_trajectories)):int(0.9 * len(processed_trajectories))]\n",
    "test_data = processed_trajectories[int(0.9 * len(processed_trajectories)):]\n",
    "\n",
    "# save the data\n",
    "train_path = 'dataset/train_data.pkl'\n",
    "val_path = 'dataset/val_data.pkl'\n",
    "test_path = 'dataset/test_data.pkl'\n",
    "\n",
    "with open(train_path, 'wb') as file:\n",
    "    pickle.dump(train_data, file)\n",
    "\n",
    "with open(val_path, 'wb') as file:\n",
    "    pickle.dump(val_data, file)\n",
    "\n",
    "with open(test_path, 'wb') as file:\n",
    "    pickle.dump(test_data, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Done!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
